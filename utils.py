# -*- coding: utf-8 -*-
"""Utils.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1kPv_p0XVk1XlCNrvuZMWXEWJCYtMJkmI
"""

import numpy as np
import math
import networkx as nx
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from scipy.spatial.distance import cosine
from copy import deepcopy

"""Find metapath scores for 6 path structures using NUMPY 3D matrix multiplication:

1.   D-D-T
2.   D-D-D-T
3.   D-T-T
4.   D-T-T-T
5.   D-D-T-T
6.   D-T-D-T

The multiplying path similarity or interaction scores obtained by matrix multiplication then
2 types of features are extracted:
sum of path scores (accumulated scores) and max of the path scores using np.sum, np.max.
"""

# This function is used for path score of length 4 to add DD or TT matrix multiplication
def DDD_TTT_sim(simM):
    np.fill_diagonal(simM, 0)
    m = np.einsum('ij,jk->ijk', simM, simM)

    sumM = np.sum((m[:, :, None]), axis=1)
    maxM = np.max((m[:, :, None]), axis=1)

    sumM = np.squeeze(sumM)
    maxM = np.squeeze(maxM)

    return sumM, maxM

# drugs similarity matrix * training DTIs matrix
def calculate_drug_similarities_and_DTIs_meta_path(drug_similarities, DTIs, length, mul=False):
    np.fill_diagonal(drug_similarities, 0)
    m = np.einsum('ij,jk->ijk', drug_similarities, DTIs)

    if (mul):
        m = m ** (length)

    sumM = np.sum((m[:, :, None]), axis=1)
    maxM = np.max((m[:, :, None]), axis=1)
    # avgM = np.mean((m[:, :, None] ), axis = 1)

    # to convert from 3-d matrix to 2-d matrix
    sumM = np.squeeze(sumM)
    maxM = np.squeeze(maxM)
    # avgM = np.squeeze(avgM)

    return (sumM, maxM)  # ,avgM)

#  Training DTIs matrix * targets similarity matrix 
def calculate_DTIS_and_target_similarities_meta_path(target_similarities, DTIs, length, mul=False):
    np.fill_diagonal(target_similarities, 0)
    m = np.einsum('ij,jk->ijk', DTIs, target_similarities)

    if (mul):
        m = m ** (length)

    sumM = np.sum((m[:, :, None]), axis=1)
    maxM = np.max((m[:, :, None]), axis=1)
    # avgM = np.mean((m[:, :, None] ), axis = 1)

    sumM = np.squeeze(sumM)
    maxM = np.squeeze(maxM)
    # avgM = np.squeeze(avgM)

    return (sumM, maxM)  # , avgM)

def metaPath_DDTT(DT, drug_similarities, target_similarities, mul=False):
    sumDDT, maxDDT = calculate_drug_similarities_and_DTIs_meta_path(drug_similarities, DT, 3, mul)
    sumDDTT, _ = calculate_DTIS_and_target_similarities_meta_path(target_similarities, sumDDT, 3, mul)
    _, maxDDTT = calculate_DTIS_and_target_similarities_meta_path(target_similarities, maxDDT, 3, mul)

    return sumDDTT, maxDDTT

def metaPath_DTDT(DT):
    TD = np.transpose(DT)
    DD = DT.dot(TD)

    sumM, maxM = calculate_drug_similarities_and_DTIs_meta_path(DD, DT, 3)

    return sumM, maxM

"""SNF functions that are needed to integrate multiple similarites."""

def FindDominantSet(W, K):
    m, n = W.shape
    DS = np.zeros((m, n))
    for i in range(m):
        index = np.argsort(W[i, :])[-K:]  # get the closest K neighbors
        DS[i, index] = W[i, index]  # keep only the nearest neighbors

    # normalize by sum
    B = np.sum(DS, axis=1)
    B = B.reshape(len(B), 1)
    DS = DS / B
    return DS

def normalized(W, alpha):
    m, n = W.shape
    W = W + alpha * np.identity(m)
    return (W + np.transpose(W)) / 2

def SNF(Wall, K, t, alpha=1):
    C = len(Wall)
    m, n = Wall[0].shape

    for i in range(C):
        B = np.sum(Wall[i], axis=1)
        len_b = len(B)
        B = B.reshape(len_b, 1)
        Wall[i] = Wall[i] / B
        Wall[i] = (Wall[i] + np.transpose(Wall[i])) / 2

    newW = []

    for i in range(C):
        newW.append(FindDominantSet(Wall[i], K))

    Wsum = np.zeros((m, n))
    for i in range(C):
        Wsum += Wall[i]

    for iteration in range(t):
        Wall0 = []
        for i in range(C):
            temp = np.dot(np.dot(newW[i], (Wsum - Wall[i])), np.transpose(newW[i])) / (C - 1)
            Wall0.append(temp)

        for i in range(C):
            Wall[i] = normalized(Wall0[i], alpha)

        Wsum = np.zeros((m, n))
        for i in range(C):
            Wsum += Wall[i]

    W = Wsum / C
    B = np.sum(W, axis=1)
    B = B.reshape(len(B), 1)
    W /= B
    W = (W + np.transpose(W) + np.identity(m)) / 2
    return W

"""Additional functions for similarity integration"""

def avg_matrix(sim_matrices):
    return np.mean(sim_matrices, axis=0)

def max_matrix(sim_matrices):
    return np.max(sim_matrices, axis=0)

def choose_negative_pairs(all_negative_pairs, DTIs, fused_DD_similarities, fused_TT_similarities, length):
    all_neg_pairs_similarity_score = {}

    for drug, target in all_negative_pairs:
        tt_similarity_sum = 0
        for t in range(fused_TT_similarities.shape[0]):
            if DTIs[t][drug] == 1:
                tt_similarity_sum += fused_TT_similarities[target][t]

        dd_similarity_sum = 0
        for d in range(fused_DD_similarities.shape[0]):
            if DTIs[target][d] == 1:
                dd_similarity_sum += fused_DD_similarities[drug][d] 

        exponent = -(tt_similarity_sum + dd_similarity_sum)
        all_neg_pairs_similarity_score[(drug, target)] = math.exp(exponent)
    
    all_neg_pairs_similarity_score_sorted = sorted(all_neg_pairs_similarity_score.items(), key=lambda kv: kv[1], reverse=True)
    chosen_negative_pairs = all_neg_pairs_similarity_score_sorted[:length]
    chosen_negative_pairs = [pair[0] for pair in chosen_negative_pairs]
    return chosen_negative_pairs

def calculate_statistics(graph):
    print("Nodes:", graph.number_of_nodes())
    print("Edges:", graph.number_of_edges())
    g = graph.to_undirected()
    number_of_components = nx.number_connected_components(g)
    components_size = [len(c) for c in sorted(nx.connected_components(g), key=len, reverse=True)]
    largest_cc = max(nx.connected_components(g), key=len)
    largest_cc_graph = graph.subgraph(largest_cc).copy()
    largest_cc_graph_nodes = largest_cc_graph.number_of_nodes()
    largest_cc_graph_edges = largest_cc_graph.number_of_edges()
    print("Number of connected components:", number_of_components)
    print("Size of each component (number of nodes):", components_size)
    print("Nodes in largest connected component:", largest_cc_graph_nodes)
    print("Edges in largest connected component:", largest_cc_graph_edges)
    plt.hist([x[1] for x in list(nx.degree(graph))])
    plt.yscale('log')
    plt.show()

def save_embeddings(file_path, embs, nodes):
    """Save node embeddings

    :param file_path: path to the output file
    :type file_path: str
    :param embs: matrix containing the embedding vectors
    :type embs: numpy.array
    :param nodes: list of node names
    :type nodes: list(int)
    :return: None
    """
    with open(file_path, 'w') as f:
        f.write(f'{embs.shape[0]} {embs.shape[1]}\n')
        for node, emb in zip(nodes, embs):
            f.write(f'{node} {" ".join(map(str, emb.tolist()))}\n')

def read_embeddings(file_path):
    """ Load node embeddings

    :param file_path: path to the embedding file
    :type file_path: str
    :return: dictionary containing the node names as keys
    and the embeddings vectors as values
    :rtype: dict(int, numpy.array)
    """
    with open(file_path, 'r') as f:
        f.readline()
        embs = {}
        line = f.readline().strip()
        while line != '':
            parts = line.split()
            embs[int(parts[0])] = np.array(list(map(float, parts[1:])))
            line = f.readline().strip()
    return embs

# get the strongest k similarities for each node in the graph
def get_strongest_k_sim(similarity_mat, k):
    m, n = similarity_mat.shape

    new_similarity_mat = np.zeros((m, n))
    for i in range(m):
        index = np.argsort(similarity_mat[i, :])[-k:]  # sort based on strongest k edges
        new_similarity_mat[i, index] = similarity_mat[i, index]  # keep only the nearest neighbors (strongest k edges)

    np.fill_diagonal(new_similarity_mat, 1)

    return new_similarity_mat

# get the strongest p% similarities from all edges in the graph 
def get_strongest_all_p_similarities(similarity_mat, p):
    m = similarity_mat.shape[0]
    num_strongest_similarities = m * (m - 1) * p // 200
    strongest_similarities_edges = {}
    for i in range(m - 1):
        for j in range(i + 1, m):
            strongest_similarities_edges[(i, j)] = similarity_mat[i][j]
    
    strongest_similarities_edges = sorted(strongest_similarities_edges.items(), key=lambda kv: kv[1], reverse=True)
    strongest_similarities_edges = strongest_similarities_edges[:num_strongest_similarities]
    strongest_similarities_edges = [pair[0] for pair in strongest_similarities_edges]
    return strongest_similarities_edges

def cosine_similarity(v1, v2):
    return 1 - cosine(v1, v2)

def Pearson_correlation_similarity(v1, v2):
  return 1 - (1 - np.corrcoef(v1, v2)[0, 1]) / 2

def normalize_matrix(matrix):
    scaler = MinMaxScaler(copy=True, feature_range=(0, 1))
    scaler.fit(matrix)
    norm_mat = scaler.transform(matrix)

    return norm_mat

def validate_file(file_path):
  file = np.loadtxt(file_path, dtype=str)
  for line in file:
    if 'nan' in line:
      print("NaN found on line:", line)
      raise RuntimeError(f"File {file_path} is invalid.")
  print(f"File {file_path} is valid.")

def mask_test_edges(test_fold_edges, test_fold_labels, DTI, num_drugs):
    DTI_train = deepcopy(DTI)
    # get the drug index and target index 
    # mask drug, target = 1 of test data to be 0 (i.e. remove the edge)
    for edge, label in zip(test_fold_edges, test_fold_labels):
        if label == 1:
            DTI_train[edge[0], edge[1] - num_drugs] = 0
    return DTI_train